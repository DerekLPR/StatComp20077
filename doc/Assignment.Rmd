---
title: "Assignment"
author: "20077 Peiran Liu"
date: "2020/12/19"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Assignment}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# 2020-9-22


## Question

Use knitr to produce 3 examples in the book.The 1st example should contain texts and at least one figure. The 2nd example should contain texts and at least one table. The 3rd example
should contain at least a couple of LaTeX formulas.

## Answer

This is a test Rmd file with figures,tables and LaTeX formulas,which helps author familar with R Studio and English.As there is no requirement for content,examples are irrelevant with math and statistics.

### Example 1:Figure
When we want to insert pictures,there are two ways to complete.

(1)The drawing of the function in R Studio.There are two function images about trigonometric functions,which are drawing by R codes.

```{r}
curve(sin(x), 0, 4*pi)
abline(h=0, lty=3)
curve(cos(x), 0, 4*pi)
abline(h=0, lty=3)
```

(2)Type the path and file name of the picture 

There are some pictures about domestic smartphones,which are on sale this year.

```{r,eval=FALSE,fig.width=10/2.54, fig.height=10/2.54, out.width="45%"}
##The figure is hard to use in R packages,so eval=FALSE 
knitr::include_graphics("smartphone/mi10u.png")
knitr::include_graphics("smartphone/P40.png")
knitr::include_graphics("smartphone/findX2Pro.png")
```

### Example 2:Table

When we want to write articles,it's hard for us to output formula by Word,especially Greek letter.As a solution,LaTeX is a useful software.There is a table about a part of Greek letters with corresponding LaTeX codes.


Greek letter     code
-----------      ----
$\alpha$          alpha
$\beta$           beta
$\gamma$          gamma
$\delta$          delta
$\sigma$          sigma
$\theta$          theta
$\mu$             mu
$\rho$            rho  
$\lambda$         lambda
$\zeta$           zeta
$\psi$            psi
$\epsilon$        epsilon

In addition,when we have to output some Mathematical symbols,LaTeX is also a practical tool.


Mathematical symbol     code
-------------------     ----
$\int_{min}^{max}$      int_{min}^{max}
$\frac{a}{b}$           frac{a}{b}
$\in$                   in
$\subset$               subset
$\prod$                 prod
$\sum$                  sum
$\neq$                  neq
$\cap$                  cap
$\cup$                  cup
$\to$                   to


### Example 3:LaTeX Formula

To make readers pay attention to form(Output LaTeX Formula) rather than contents,the author just show a simple math problem.

Get the limit of the mathematical expression
\[
\lim_{x\to 0} \frac{e^x-1}{\sin x}
\]

Solution.

(1)(L'Hospital's rule)
\[
\lim_{x\to 0} \frac{e^x-1}{\sin x}=\lim_{x\to 0} \frac{(e^x-1)'}{(\sin x)'}=\lim_{x\to 0} \frac{e^x}{\cos x}=1
\]

(2)(substitution of equivalence infinitesimal)

According to basic knowledge of calculus,when $x\to 0$
\[
e^x\to x,\sin x\to x
\]
As a result
\[
\lim_{x\to 0} \frac{e^x-1}{\sin x}=1
\]



# 2020 9-29

### Caption
When generate the random sample,the results of each time are different.So sometimes the  the empirical and theoretical distributions are different,so the density density histogram(empirical distribution) may not coincide with density curve(theoretical distribution).

### Question1(3.3)
The Pareto$(a, b)$ distribution has cdf
\[
F(x)=1-\left(\frac{b}{x}\right)^a,x>=b>0,a>0.
\]
Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse
transform method to simulate a random sample from the Pareto$(2, 2)$ distribution. Graph the density histogram of the sample with the Pareto$(2, 2)$
density superimposed for comparison.




### Answer1
\[
u=1-(\frac{b}{x})^a\Rightarrow(\frac{b}{x})^a=1-u\Rightarrow\frac{b}{x}=(1-u)^\frac{1}{a}\Rightarrow x=\frac{b}{(1-u)^\frac{1}{a}}
\]
the probability inverse transformation$F^{-1}(u)=\frac{b}{(1-u)^\frac{1}{a}}$

as $a=2,b=2$
\[
x=F^{-1}(u)=\frac{2}{(1-u)^\frac{1}{2}}=\sqrt{\frac{4}{1-u}}
\]
the pdf is
\[
f(x)=F'(x)=\frac{8}{x^3}
\]

```{r}
n <- 1000
u <- runif(n)
x <- sqrt(4/(1-u)) #simulate a random sample from the Pareto(2, 2) distribution.
hist(x, prob = TRUE, main = expression(f(x)==8/x^{3}))#density histogram of sample
y <- seq(2, 25, .01)
lines(y, (2/y)^3) #density curve pdf
```


### Question2(3.9)
 The rescaled Epanechnikov kernel  is a symmetric density function.
 \[
 f_e(x)=\frac{3}{4}(1-x^2), |x|\leq1
 \]
 Devroye and Gyorfi  give the following algorithm for simulation
 from this distribution. Generate i.i.d $U_1, U_2, U_3$∼ Uniform(−1, 1). If $|U_3|\geq|U_2|$ and $|U_3|\geq|U_1|$, deliver $U_2$; otherwise deliver $U_3$. Write a function
 to generate random variates from $f_e$, and construct the histogram density
 estimate of a large simulated random sample.


### Answer2
Using the acceptance-rejection method.

the pdf of random variable $Y$ is$f(y)=\frac 34(1-y^2)$,the random variable $X \sim U(-1,1)$,pdf is$g(x)=\frac 12$,so
\[
\frac{f(x)}{g(x)}=\frac{3}{2}(1-x^2)\leq \frac 32
\]
mark $c=\frac 32$.Random variable $U\sim U(0,1)$.When $u<\frac{f(x)}{cg(x)}$,accept $x$ and deliver $y=x$;else reject $x$. 


```{r}
n <- 1000
k <- 0 #counter for accepted
j <- 0 #iterations,it's not necessary
y <- numeric(n)
while (k < n) {
u <- runif(1)
j <- j + 1
z <- runif(1) 
x <- 2*z-1 #random variate from g(x)
if ( (1-x^2) >u ) {#when accept x
#we accept x
k <- k + 1
y[k] <- x
}
}
j#the number of variable X
hist(y, prob = TRUE)#the histogram of Y
```


### Question3(3.10)
 Prove that the algorithm given in Exercise 3.9 generates variates from the
density $f_e$.

### Answer3

As $U_1,U_2,U_3$i.i.d. ~$U(-1,1)$ mark random variable Y is
\[
Y=U_2,\text{when} |U_3|\geq|U_2| \text{and} |U_3|\geq|U_1|,\\
Y=U_3,\text{others}
\]
so the pdf of Y is
\[
f_Y(x)=f(Y=U_2,U_2=x)+f(Y=U_3,U_3=x)
\]
Calculate two functions
\[
f(Y=U_2,U_2=x)=f(|U_3|\geq|U_2|,|U_3|\geq|U_1|,U_2=x)\\
=(\int_{-1}^{-|x|}+\int_{|x|}^{1})du_3(\int_{-|u_3|}^{|u_3|}\frac 18 du_1)\\
=(\int_{-1}^{-|x|}+\int_{|x|}^{1})\frac{|u_3|}{4}du_3\\
=\frac{1-x^2}{4}
\]

\[
f(Y=U_3,U_3=x)=f(U_3=x)-f(U_3=x,Y\not= U_3)\\
=f(U_3=x)-f(|U_3|\geq|U_2|,|U_3|\geq|U_1|,U_3=x)\\
=\int_{-1}^{1}d u_1\int_{-1}^{1} \frac{1}{8}d u_2-\int_{-|x|}^{|x|}d u_1\int_{-|x|}^{|x|} \frac{1}{8}d \\
=\frac{1-x^2}{2}
\]
so the pdf of Y is
\[
f_Y(x)=f(Y=U_2,U_2=x)+f(Y=U_3,U_3=x)=\frac 34(1-x^2)=f_e(x)
\]
So the algorithm given in Exercise 3.9 generates variates from the
density $f_e$.


### Question4(3.13)
It can be shown that the mixture in Exercise 3.12 has a Pareto distribution
with cdf
\[
F(y)=1-(\frac{\beta}{\beta+y})^{\gamma},y\geq0
\]

(This is an alternative parameterization of the Pareto cdf given in Exercise
3.3.) Generate 1000 random observations from the mixture with $\gamma = 4$ and$\beta = 2$. Compare the empirical and theoretical (Pareto) distributions by graphing the density histogram of the sample and superimposing the Pareto density
curve.

### Answer4
Assume $x=y+\beta\rightarrow y=x-\beta$,according to the question 1,probability inverse transformation of  $x$ is$x=\frac{\beta}{(1-u)^\frac{1}{\gamma}}$,so probability inverse transformation of $y$ is
\[y=F^{-1}(u)=\frac{\beta}{(1-u)^\frac{1}{\gamma}}-\beta.
\]
as $\gamma=4,\beta=2$.
\[
y=\frac{2}{(1-u)^\frac{1}{4}}-2=(\frac{16}{1-u})^\frac {1}{4}-2.
\]
the pdf is
\[
f(y)=F'(y)=\frac{64}{(2+y)^5}
\]


```{r}
n <- 1000
u <- runif(n)
x <- (16/(1-u))^(1/4)-2 #simulate a random sample from the Pareto(2, 2) distribution with 1000 observations
hist(x, prob = TRUE, main = expression(f(x)==64/(2+x)^5))#density histogram of sample(empirical distribution)
y <- seq(0, 4, .01)
lines(y, 2*(2/(y+2))^5)#density curve pdf(theoretical distribution)
```


The histogram and density plot  suggests that the empirical and
theoretical distributions approximately agree.


# 2020-10-13

### Question 1(5.1)

 Compute a Monte Carlo estimate of
 \[
\int_{0}^{\frac{\pi}{3}}\sin t dt
\]
and compare your estimate with the exact value of the integral.

### Answer
\[
\int_{0}^{\frac{\pi}{3}}\sin t dt=-\cos t|_{t=0}^{t=\frac {\pi}{3}}=1-\frac 12=0.5
\]
So the exact value of the integral is $0.5$.

```{r}
set.seed(79)
m <- 100000
x <- runif(m)##x~U(0,1)
t <- x*pi/3##t~U(0,pi/3)
theta.hat <- mean(sin(t))## the estimate
print(theta.hat)##output the estimate 
```

The estimate is 0.4784859,the exact value of the integral is $0.5$.

```{r}
print((theta.hat-0.5)/0.5)##the percent of bias
```

### Question 2(5.6&&5.7)
(5.6) In Example 5.7 the control variate approach was illustrated for Monte Carlo
integration of
\[
\theta=\int_0^1 e^xdx
\]
Now consider the antithetic variate approach. Compute $Cov (e^U , e^{1−U} )$ and
$Var(e^U + e^{1−U} )$, where $U\sim Uniform(0,1)$. What is the percent reduction in
variance of \hat{c} that can be achieved using antithetic variates (compared with
simple MC)?

(5.7) Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate θ by the
antithetic variate approach and by the simple Monte Carlo method. Compute
an empirical estimate of the percent reduction in variance using the antithetic
variate. Compare the result with the theoretical value from Exercise 5.6.



### Answer
\[
E(e^U+e^{1-U})=\int_0^1e^u+e^{1-u}du=(e^u-e^{1-u})|_{u=0}^{u=1}=2e-2
\]
\[
E(e^U)=\int_0^1e^udu=(e^u)|_{u=0}^{u=1}=e-1
\]
\[
E(e^U)=E(e^U+e^{1-U})-E(e^U)=e-1
\]
Compute the Variance
\[
Var(e^U+e^{1-U})=E[(e^U+e^{1-U})^2]-[E(e^U+e^{1-U})]^2\\
=\int_0^1(e^u+e^{1-u})^2du-(2e-2)^2\\
=(\frac{e^{2u}}{2}-\frac{e^{2-2u}}{2}+2eu)|_{u=0}^{u=1}-4e^2+8e-4\\
=-3e^2+10e-5
\]
\[
Var(e^U)=E[(e^U)^2]-[E(e^U)]^2\\
=\int_0^1(e^u)^2du-(e-1)^2\\
=(\frac{e^{2u}}{2})|_{u=0}^{u=1}-e^2+2e-1\\
=-\frac 12e^2+2e-\frac 32
\]
\[
Var(e^{1-U})=E[(e^{1-U})^2]-[E(e^{1-U})]^2\\
=\int_0^1(e^{1-u})^2du-(e-1)^2\\
=(-\frac{e^{2-2u}}{2})|_{u=0}^{u=1}-e^2+2e-1\\
=-\frac 12e^2+2e-\frac 32
\]
Compute the covariance
\[
Cov(e^U,e^{1-U})=\frac 12[Var(e^U+e^{1-U})-Var(e^U)-Var(e^{1-U})]\\
=-2e^2+6e-2
\]

```{r}
print(-2*exp(2)+6*exp(1)-2)## the theoretical Covariance
```

```{r}
print(-3*exp(2)+10*exp(1)-5)##the theoretical Variance
```
for the Antithetic Variables$(e^U+e^{1-U})/2$,the variance is 

```{r}
print((-3*exp(2)+10*exp(1)-5)/4)
```

```{r}
set.seed(20077)
n <-10000
u <-runif(n)
MC.simple <-exp(u)##the smaple of Simple Monte Carlo
Mc.antithetic <-(exp(u)+exp(1-u))/2##the smaple of Antithetic Variables
theta.s <-mean(MC.simple)##the estimate of  Simple Monte Carlo
theta.a <-mean(Mc.antithetic)##the estimate of Antithetic Variables
sd.s <-sd(MC.simple)##the sd of Simple Monte Carlo
sd.a <-sd(Mc.antithetic)##the sd of Antithetic Variables
print(theta.s)##the estimate of  Simple Monte Carlo
print(theta.a)##the estimate of  Simple Monte Carlo
print(sd.s)##the sd of Simple Monte Carlo
print(sd.a)##the sd of Antithetic Variables
print((sd.s-sd.a)/sd.s)##the percent reduction in variance using the antithetic
```
The theoretical value is$\theta=\int_0^1 e^x dx=e-1$


```{r}
print(exp(1)-1)##the theoretical value
```

The theoretical variance of the antithetic variables is 0.003912497,the sd is  0.0621996,the empirical estimate of the percent reduction in variance using the antithetic
variate is 87.24%.


### Question  3(5.11)

 If $\theta_1$ and $\theta_2$ are unbiased estimators of $\theta$, and $\hat{\theta}_1$ and $\hat{\theta}_1$ are antithetic, we
derived that $c^*=\frac 12$ is the optimal constant that minimizes the variance of
$\hat{\theta}_c = c\hat{\theta}_1 + (1 − c)\hat{\theta}_2$. Derive $c^*$ for the general case. That is, if $\hat{\theta}_1$ and $\hat{\theta}_2$
are any two unbiased estimators of θ, find the value $c^*$ that minimizes the
variance of the estimator $\hat{\theta}_c = c\hat{\theta}_1 + (1 − c)\hat{\theta}_2$in equation (5.11). ($c^*$ will be
a function of the variances and the covariance of the estimators.)


### Answer

The equation (5.11) is
\[
Var(\hat{\theta}_c)=Var(\hat{\theta}_2)+c^2Var(\hat{\theta}_1-\hat{\theta}_2)+2cCov(\hat{\theta}_2,\hat{\theta}_1-\hat{\theta}_2)
\]
For $\forall$ random variable $X$,$Var(X)\geq 0$.In general case,$Var(X)\not =0$,so $Var(X)>0$.So
\[
Var(\hat{\theta}_c)=Var(\hat{\theta}_1-\hat{\theta}_2)[c+\frac{Cov(\hat{\theta}_2,\hat{\theta}_1-\hat{\theta}_2)}{Var(\hat{\theta}_1-\hat{\theta}_2)}]^2+Var(\hat{\theta}_2)-\frac{[Cov(\hat{\theta}_2,\hat{\theta}_1-\hat{\theta}_2)]^2}{Var(\hat{\theta}_1-\hat{\theta}_2)}
\]
So when $c+\frac{Cov(\hat{\theta}_2,\hat{\theta}_1-\hat{\theta}_2)}{Var(\hat{\theta}_1-\hat{\theta}_2)}=0$,$Var(\hat{\theta}_c)$ can get the minimum value$Var(\hat{\theta}_2)-\frac{[Cov(\hat{\theta}_2,\hat{\theta}_1-\hat{\theta}_2)]^2}{Var(\hat{\theta}_1-\hat{\theta}_2)}$.So
\[
c^*=-\frac{Cov(\hat{\theta}_2,\hat{\theta}_1-\hat{\theta}_2)}{Var(\hat{\theta}_1-\hat{\theta}_2)}=\frac{Var(\hat{\theta}_2,\hat{\theta}_2)-Cov(\hat{\theta}_2,\hat{\theta}_1)}{Var(\hat{\theta}_1)+Var(\hat{\theta}_2)-2Cov(\hat{\theta}_2,\hat{\theta}_1)}
\]



# 2020-10-20

### Question 1
Find two importance functions $f_1$ and $f_2$ that are supported on $(1,\infty)$ and
are ‘close’ to
\[
g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}},x>1
\]
Which of your two importance functions should produce the smaller variance
in estimating
\[
\int_{1}^{\infty}\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx
\]
by importance sampling? Explain

### Answer

\[
f_1(x)=I(x>1)\frac{xe^{-\frac{x^2}{2}}}{e^{-\frac 12}}=I(x>1)e^{\frac 12}xe^{-\frac{x^2}{2}}
\]
So 
\[
\int_{1}^{\infty} f_1(x)dx=-e^{\frac 12}e^{-\frac{x^2}{2}}|_{x=1}^{x=\infty}=1
\]

\[
f_2(x)=I(x>1)\frac{e}{5}x^2e^{-x}
\]
And 
\[
\int_{1}^{\infty}f_2(x)dx=\int_{1}^{\infty}\frac{e}{5}x^2e^{-x}dx
=1\]

\[
\frac{g(x)}{f_1(x)}=\frac{x}{\sqrt{2e\pi}},\frac{g(x)}{f_2(x)}=\frac{5}{\sqrt{2\pi}e}e^{x-\frac{x^2}{2}}
\]

```{r}
set.seed(20077)
n <- 10000
theta.hat <- se <- numeric(2)
gf_1 <- numeric(n)
gf_2 <- numeric(n)
gf1 <- function(z) {##function g/f1
z/sqrt(2*pi*exp(1))
}
gf2 <- function(z) {##function g/f2
5*exp(z-z^2/2)/sqrt(2*pi*exp(2))
}
y <- runif(n)##y~U(0,1)
x <- 1/y##x~U(1,\infty)
gf_1 <- gf1(x)
gf_2 <- gf2(x)
theta.hat[1] <- mean(gf_1)
theta.hat[2] <- mean(gf_2)
se[1] <- sd(gf_1)
se[2] <- sd(gf_2)
rbind(theta.hat, se)
```
the variance of $gf_2$ is smaller than $gf_1$,the $gf_2$ is better choice.

### Question 2

Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.




### Answer

In each interval $(\frac{k-1}{5},\frac{k}{5}),k=1,2,3,4,5$
\[
h_k(x)=I(\frac{k-1}{5}<x<\frac{k}{5})\frac{e^{-x}}{e^{-\frac{k-1}{5}}-e^{-\frac{k}{5}}},k=1,2,3,4,5
\]
So the integral
\[
\int_{\frac{k-1}{5}}^{\frac{k}{5}}h_k(x)dx=\frac{1}{e^{-\frac{k-1}{5}}-e^{-\frac{k}{5}}}\int_{\frac{k-1}{5}}^{\frac{k}{5}}e^{-x}dx=1
\]
\[
\frac{g(x)}{h_k(x)}=\frac{e^{-\frac{k-1}{5}}-e^{-\frac{k}{5}}}{1+x^2}
\]

```{r}
set.seed(20077)
M <- 50000##number of replicates
theta.hat <- var.hat<- numeric(5)
h <- function(x,k){(exp(-(k-1)/5)-exp(-(k)/5))/(1+x^2)}##importance function on each subinterval
for(i in 1:5){
  x <- runif(M/5, (i-1)/5, i/5)
  y <- h(x,i)
  theta.hat[i]=mean(y)##the estimate on each subinterval
  var.hat[i]=var(y)##the variance on each subinterval
}
theta_hat=sum(theta.hat)
var_hat=sum(var.hat)
theta_hat##output the estimate
var_hat##output the variance
sqrt(var_hat)##the standard deviation
```

The standard deviation of stratified smportance sampling is 0.007919952,which is smaller than the standard deviation 0.09658794 in Example 5.10

### Question 3
 Suppose that $X_1,...,X_n$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for
the parameter $\mu$. Use a Monte Carlo method to obtain an empirical estimate
of the confidence level.


### Answer

$X$：lognormal distribution,$\ln X\sim N(\mu,\sigma^2)$,the pdf is
\[
f(x;\mu,\sigma)=I(x>0)\frac{1}{x\sqrt{2\pi}\sigma}e^{-\frac{(\ln x-\mu)^2}{2\sigma^2}}
\]
As $\ln X\sim N(\mu,\sigma^2)$ and $\sigma^2$ is unkown,mark$Y=\ln X\sim N(\mu,\sigma^2)$.

According to the knowledge from mathematical statistics,mark $S_Y$
\[
S_Y^2=\frac{1}{n-1}\sum_{i=1}^{n}(Y_i-\overline{Y})^2
\]
So the variable $T$
\[
T=\frac{\sqrt{n}(\overline{Y}-\mu)}{S_Y}\sim t_{n-1}
\]
\[
P(|T|\leq c)=P(-c\leq \frac{\sqrt{n}(\overline{Y}-\mu)}{S_Y}\leq c)=1-\alpha
\]
If $c=t_{n-1}(\frac{\alpha}{2})$,the $1-\alpha$ confidence interval for
the parameter $\mu$ is 
\[
\left[\overline{Y}-\frac{S_Y}{\sqrt{n}}t_{n-1}(\frac{\alpha}{2}),\overline{Y}+\frac{S_Y}{\sqrt{n}}t_{n-1}(\frac{\alpha}{2}) \right]
\]
And$P(t_{n-1}>t_{n-1}(\alpha))=\alpha$

When $\alpha=0.05$,the 95% confidence interval is
\[
\left[\overline{Y}-\frac{S_Y}{\sqrt{n}}t_{n-1}(0.025),\overline{Y}+\frac{S_Y}{\sqrt{n}}t_{n-1}(0.025) \right]
\]

```{r}
set.seed(20077)
n <- 25##Sample size of each experiments
m <- 10000##Number of experiments
alpha <- 0.05
Y <- matrix(c(0:0) ,nrow=m , ncol=n)
t <- qt(1-(alpha/2),n-1)##t(alpha/2),use qt function
te <- c(1:m)## te=1 if mu in confidence region,else te=0
for (i in 1:m) {
  Y[i,] <- rnorm(n , mean=0 , sd=1)##normal distribution
  mu <- mean(Y[i,]) 
  se <- sqrt(var(Y[i,]))
  min_Y <- mu-se*t/sqrt(n)##lower limit of confidence region
  max_Y <- mu+se*t/sqrt(n)##upper limit of confidence region
  if(0<min_Y) {te[i]=0}
  else if(0>max_Y) {te[i]=0}
  else {te[i]=1}## te=1 if mu in confidence region,else te=0
}
conlevel <- mean(te)##empirical estimate of the confidence level
print(conlevel)##print empirical estimate of the confidence level
cat("[",min_Y,",",max_Y,"]") ##One of the confidence levels
```

The empirical estimate of the confidence level is 0.9447,here is a 95% confidence interval for the parameter $\mu$.
\[
[-0.0966212,0.6970358]
\]

### Question 4
 Suppose a 95% symmetric t-interval is applied to estimate a mean, but the
sample data are non-normal. Then the probability that the confidence interval
covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment
to estimate the coverage probability of the t-interval for random samples of
$\chi^2(2)$ data with sample size $n = 20$. Compare your t-interval results with the
simulation results in Example 6.4. (The t-interval should be more robust to
departures from normality than the interval for variance.)

### Answer

```{r}
set.seed(20077)
n <- 20##Sample size of each experiments
m <- 10000##Number of experiments
alpha <- 0.05
Y <- matrix(c(0:0) ,nrow=m , ncol=n)
t <- qt(1-(alpha/2),n-1)##t(alpha/2),use qt function
te <- c(1:m)## te=1 if mu in confidence region,else te=0
for (i in 1:m) {
  Y[i,] <- rchisq(n , df=2)##chi-square distribution
  mu <- mean(Y[i,]) 
  se <- sqrt(var(Y[i,]))
  min_Y <- mu-se*t/sqrt(n)##lower limit of confidence region
  max_Y <- mu+se*t/sqrt(n)##upper limit of confidence region
  if(2<min_Y) {te[i]=0}
  else if(2>max_Y) {te[i]=0}
  else {te[i]=1}## te=1 if mu in confidence region,else te=0
}
conlevel <- mean(te)##t-interval results
print(conlevel)##print t-interval results
```


```{r}
set.seed(20077)
m <- 10000##Number of experiments
n <- 20##Sample size of each experiments
alpha <- .05
test <- numeric(m)
for(i in 1:m){
  x <- rnorm(n, mean=0, sd=2)##normal distribution
  UCL <- (n-1) * var(x) / qchisq(alpha, df=n-1)## the upper confidence limit
  if(4<UCL) test[i]<-1
  else test[i]<-0## te=1 if mu in confidence region,else te=0
}
t<-mean(test)
t##simulation results in Example 6.4
```


Simulation results is 0.9456,t-interval results is 0.9173.The t-interval is more robust to departures from normality than the interval for variance.



# 2020-10-27

### Question 1(6.7)

Estimate the power of the skewness test of normality against symmetric
Beta$(\alpha,\alpha)$ distributions and comment on the results. Are the results different
for heavy-tailed symmetric alternatives such as $t(\nu)$?


### Answer 

```{r}
set.seed(20077)
alpha <- 0.1
m <- 1000
n <- 20 ##Sample size
a <- c(seq(0.05,1,0.05),seq(1,97,4))##parameter of beta distribution
N <- length(a)
power_sk <- numeric(N)
power_t <- numeric(N)
cv <- qnorm(1-alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
cv_t <- qt(1-alpha,n-1)

skew <- function(y){
  mean_y <- mean (y)
  m_2 <- mean((y-mean_y)^2)
  m_3 <- mean((y-mean_y)^3)
  return(sqrt(((m_3)^2)/((m_2)^3)))
}

ft <- function(z){
  m1<-mean(z)
  m2<-mean((z-m1)^2)
  return(sqrt(20/m2)*(m1-0.5))
}

for (j in 1:N) { 
para <- a[j]
sktests <- numeric(m)
for (i in 1:m) { 
x <- rbeta(n,para,para)
sktests[i] <- as.integer(abs(skew(x)) > cv)
}
power_sk[j] <- mean(sktests)

}

plot(a, power_sk, type = "b",
xlab = bquote(a), ylim = c(0,1))
abline(h = .1, lty = 3)
se_sk <- sqrt(power_sk * (1-power_sk) / m) 
lines(a, power_sk+se_sk, lty = 3)
lines(a, power_sk-se_sk, lty = 3)

for (j in 1:N) { 
para <- a[j]
t_test <- numeric(m)
for (i in 1:m) { 
x <- rbeta(n,para,para)
t_test[i] <- as.integer( ft(x) > cv_t)
}
power_t[j] <- mean(t_test)
}

plot(a, power_t, type = "b",
xlab = bquote(a), ylim = c(0,1))
abline(h = .1, lty = 3)
se_t <- sqrt(power_t * (1-power_t) / m) 
lines(a, power_t+se_t, lty = 3)
lines(a, power_t-se_t, lty = 3)
```

Comment:the power don't change a lot with the change of parameter.

The result is a little similar to the result in t test.

### Question 1(6.8)

Refer to Example 6.16. Repeat the simulation, but also compute the F test
of equal variance, at significance level $\hat{\alpha}=$ 0.055. Compare the power of the
Count Five test and F test for small, medium, and large sample sizes. (Recall
that the F test is not applicable for non-normal distributions.)


### Answer

```{r}
set.seed(20077)
m<- 10000
n <- c(20,100,500)
alpha <- 0.055

c5test <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
return(as.integer(max(c(outx, outy)) > 5))
}




Ftest <- function(x, y, i) {
V_x  <- var(x)
V_y  <- var(y)
k <- c(20,100,500)
qF <- c(qf(1-alpha,k[1],k[1]),qf(1-alpha,k[2],k[3]),qf(1-alpha,k[3],k[3]))
return(as.integer( V_x/V_y > qF[i]))
}


sigma1 <- 1
sigma2 <- 1.1
power_c5<-numeric(3)
power_F<-numeric(3)
pF<- pc5<- numeric(m)
for(i in 1:3){
  for(j in 1:m){
      x <- rnorm(n[i], 0, sigma1)
      y <- rnorm(n[i], 0, sigma2)
      pF[j] <- Ftest(x,y,i)
      pc5[j] <- c5test(x,y)
  }
  power_c5[i] <- mean(pc5)
  power_F[i]<- mean(pF)
}
print(power_c5)
print(power_F)

```


### Question 1(6.c)
Repeat Examples 6.8 and 6.10 for Mardia’s multivariate skewness test. Mardia [187] proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If $X$ and $Y$ are iid, the multivariate
population skewness $\beta_{1,d}$ is defined by Mardia as
\[
\beta_{1,d}=E[(X-\mu)^T\Sigma^{-1}(Y-\mu)]^3
\]
Under normality, $\beta_{1,d}$ The multivariate skewness statistic is
\[
b_{1,d}=\frac{1}{n^2}\sum_{i,j=1}^{n}[(X_i-\overline{X})^T\hat{\Sigma}^{-1}(X_j-\overline{X})]^3
\]
where$\hat{\Sigma}$ is the maximum likelihood estimator of covariance. Large values of $b_{1,d}$ are significant. The asymptotic distribution of $nb_{1,d}/6$ is chisquared with
$d(d + 1)(d + 2)/6$ degrees of freedom.


### Answer
The maximum likelihood estimator of covariance is 
\[
\hat{\Sigma}=\sum_{i=1}^n\frac {1}{n}(X_i-\overline{X})(X_i-\overline{X})^T
\]
Assume the dimension $d$ is 2,and $X$ is standard multivariate normal distribution.

```{r}
set.seed(2007)
library("MASS")
alpha <- 0.05
m <- 1000
n <- 20
cv_1 <- qchisq(1-alpha/2,2)
cv_2 <- qchisq(alpha/2,2)
Sigma <- matrix(c(10,3,3,2),2,2)

sk <- function(x,n){
  l <- nrow(x)
  x1 <- x[,1]
  x2 <- x[,2]
  mean_X1 <- mean(x1)
  mean_X2 <- mean(x2)
  m_11 <- sum((x1-mean_X1)^2)
  m_12 <- sum((x1-mean_X1)*((x2-mean_X2)))
  m_21 <- sum((x1-mean_X1)*(x2-mean_X2))
  m_22 <- sum((x2-mean_X2)^2)
  M <- matrix(c(m_11,m_12,m_21,m_22),nrow=2,ncol=2)
  Mm<-solve(M)##Inverse matrix
  result <- 0
  for(i in 1:n){
    for(j in 1:n){
      a <- matrix(c(x1[i]-mean_X1,x2[i]-mean_X2),nrow=1,ncol=2)
       b <- matrix(c(x1[j]-mean_X1,x2[j]-mean_X2),nrow=2,ncol=1)
       result <- result+a%*%Mm%*%b 
    }
  }
  return(result)
}

re<-numeric(m)
for(j in 1:m){
  x <- mvrnorm(n=20, rep(0, 2), Sigma)
  ske <- sk(x,n)
  if(ske>cv_1) {re[j]=1}
  else if(ske<cv_2) {re[j]=1}
  else {re[j]=0}
}
p <- mean(ske)
print(p)


para <- seq(9,11,0.1)##parameter change
lp <- length(para)
power <- numeric(lp)

for(i in 1:lp){
  for(j in 1:m){
  x <- mvrnorm(n=20, rep(0, 2), matrix(c(para[i],3,3,2),2,2))
  ske <- sk(x,n)
  if(ske>cv_1) {re[j]=1}
  else if(ske<cv_2) {re[j]=1}
  else {re[j]=0}
}
power[i] <- mean(ske)
}

##Curve
plot(para, power, type = "b",
xlab = bquote(para), ylim = c(0,1))
abline(h = .1, lty = 3)
se <- sqrt(power * (1-power) / m) 
lines(para, power+se, lty = 3)
lines(para, power-se, lty = 3)
```




### Question 4

If we obtain the powers for two methods under a particular
simulation setting with 10,000 experiments: say, 0.651 for one
method and 0.676 for another method. Can we say the powers
are different at 0.05 level?

(a)What is the corresponding hypothesis test problem?

(b)What test should we use? Z-test, two-sample t-test, paired-t
test or McNemar test?

(c)What information is needed to test your hypothesis?

### Answer (a)

\[
\text{$H_0$:the powers of two sample are different}\Longleftrightarrow\text{$H_1$:the powers of two sample are same}
\]
the significance level is 0.05

### Answer (b)

As we can not that two samples are independant,it's better to use McNemar test.


### Answer (c)

Only need to know the power 0.651 and 0.676.



# 2020-11-3

### Question1 (7.1)
 Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.


### Answer

```{r}
set.seed(20077)
library(bootstrap)##the data of law
n <- nrow(law)
cor_j <- numeric(n)
cor_hat <- cor(law$LSAT,law$GPA)

for(i in 1:n){##Jackknife Sample
  law_j <- law[-i,]
  cor_j[i] <-cor(law_j$LSAT,law_j$GPA)
}

cor_jack <- mean(cor_j)
bias_jack <- (n-1)*(cor_jack-cor_hat)
se_jack <-  sqrt((n-1)/n*sum((cor_j-cor_jack)^2))


result<-matrix(c(cor_jack,bias_jack,se_jack),nrow=1,ncol=3)
rownames(result) <- c('Jackknife')
colnames(result) <- c('estimate','bias', 'se')
result

```


### Question2 (7.5)
Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the
mean time between failures $\frac{1}{\lambda}$ by the standard normal, basic, percentile,
and BCa methods. Compare the intervals and explain why they may differ.


### Answer
Compute the Maximum Likelihood Estimate of $\lambda$
\[
l(X;\lambda)=\ln L(X;\lambda)=\sum_{i=1}^n \ln f(x_i;\lambda)=n\ln\lambda-\sum_{i=1}^n \lambda x_i
\]
\[
 \frac{\partial l}{\partial \lambda}=\frac{n}{\lambda}-\sum_{i=1}^nx_i
\]
if $\frac{\partial l}{\partial \lambda}=0$,then
\[
\hat{\lambda}_{MLE}=\frac{1}{\overline{X}}
\]
So the estimate of $\frac{1}{\lambda}$ is
\[
\hat{(\frac{1}{\lambda})}=\overline{X}
\]

```{r}
set.seed(20077)
library(boot)##the data of aircondit
B <- 10000
alpha <- 0.05 
len <- nrow(aircondit)
air <- numeric(len)
for(i in 1:len){
  air[i] <- aircondit[i,]
}
time_boot<-numeric(B)
time_hat<-mean(air)


for(i in 1:B){
  air_boot<- sample(air,len,replace=TRUE)
  time_boot[i] <- mean(air_boot)
}## the Bootstrap sample


boot.n <- function(th,th_hat,conf){## normal confidence interval 
  se_hat <- sd(th)
  z <- qnorm(1-conf/2,mean=0,sd=1)
  result_norm<-matrix(c(th_hat-z*se_hat,th_hat+z*se_hat),nrow=1,ncol=2)
  colnames(result_norm) <- c('0.025','0.975')
  rownames(result_norm) <- c('norm')
  print(result_norm)
}

boot.p <- function(th,conf) {##percentile confidence interval 
quantile(th,probs = c((1-conf)/2,(1+conf)/2))
} 


boot.b <- function(th,th_hat,conf) {##basic confidence interval 
  th_minus <- 2*th_hat-th
  quantile(th_minus,probs=c((1-conf)/2,(1+conf)/2))
} 


boot.BCa <-##the BCa function from textbook
function(x, th0, th, stat, conf = .95) {
# bootstrap with BCa bootstrap confidence interval
# th0 is the observed statistic
# th is the vector of bootstrap replicates
# stat is the function to compute the statistic
x <- as.matrix(x)
n <- nrow(x) 
N <- 1:n
alpha <- (1 + c(-conf, conf))/2
zalpha <- qnorm(alpha)
z0 <- qnorm(sum(th < th0) / length(th))
th.jack <- numeric(n)
for (i in 1:n) {
J <- N[1:(n-1)]
th.jack[i] <- stat(x[-i, ], J)
}
L <- mean(th.jack) - th.jack
a <- sum(L^3)/(6 * sum(L^2)^1.5)
adj.alpha <- pnorm(z0 + (z0+zalpha)/(1-a*(z0+zalpha)))
limits <- quantile(th, adj.alpha, type=6)
return(list("est"=th0, "BCa"=limits))
}

time.boot <- function(dat, index){
# input: dat, data; index, a vector of the bootstrap index
# output: theta, the estimated theta using the bootstrap sample
air.boot <- dat[index]
time.hat<-mean(air.boot)
return(time.hat)
}



boot.n(time_boot,time_hat,1-alpha)##normal confidence interval 

boot.p(time_boot,1-alpha)##percentile confidence interval 

boot.b(time_boot,time_hat,1-alpha)##basic confidence interval 

boot.BCa(air,th0 = time_hat,th = time_boot,stat = time.boot)##BCa confidence interval 
```

As we can see,the length of normal confidence interval is smaller than the other three confidence intervals.In conclusion,the standard normal method is not a good choice.

These four methods have different purpose,they have different principle.As the methods are not same,the results are different.

### Question3 (7.8)
 Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard
error of $\hat{\theta}$.


### Answer

```{r}
set.seed(20077)
library(bootstrap) #for the score data
B<-1000
n<-nrow(scor)
m<-ncol(scor)
lambda_hat <- eigen(cov(scor))$values
theta_hat <- lambda_hat[1] / sum(lambda_hat)

theta_jack <- numeric(n)
for (i in 1:n) {
scor_jack <- scor [-i,]
lambda_jack <- eigen(cov(scor_jack))$values
theta_jack[i] <- lambda_jack[1] / sum(lambda_jack)
}
bias_hat_jack <- (n - 1) * (mean(theta_jack) - theta_hat)
se_hat_jack <-  sqrt((n-1)/n*sum((theta_jack-mean(theta_jack))^2))

result<-matrix(c(mean(theta_jack),bias_hat_jack,se_hat_jack),nrow=1,ncol=3)
rownames(result) <- c('Jackknife')
colnames(result) <- c('estimate','bias', 'se')
result

```




### Question4 (7.11)
In Example 7.18, leave-one-out (n-fold) cross validation was used to select the
best fitting model. Use leave-two-out cross validation to compare the models.


### Answer

```{r}
set.seed(20077)
library(DAAG); attach(ironslag)
a <- seq(10, 40, .1) #sequence for plotting fits
L1 <- lm(magnetic ~ chemical)
plot(chemical, magnetic, main="Linear", pch=16)
yhat1 <- L1$coef[1] + L1$coef[2] * a
lines(a, yhat1, lwd=2)
L2 <- lm(magnetic ~ chemical + I(chemical^2))
plot(chemical, magnetic, main="Quadratic", pch=16)
yhat2 <- L2$coef[1] + L2$coef[2] * a + L2$coef[3] * a^2
lines(a, yhat2, lwd=2)
L3 <- lm(log(magnetic) ~ chemical)
plot(chemical, magnetic, main="Exponential", pch=16)
logyhat3 <- L3$coef[1] + L3$coef[2] * a
yhat3 <- exp(logyhat3)
lines(a, yhat3, lwd=2)
L4 <- lm(log(magnetic) ~ log(chemical))
plot(log(chemical), log(magnetic), main="Log-Log", pch=16)
logyhat4 <- L4$coef[1] + L4$coef[2] * log(a)
lines(log(a), logyhat4, lwd=2)


n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n*(n-1)/2)
# for n-fold cross validation
# fit models on leave-one-out samples

k <- 0

for (j in 2:n) {
  for(i in 1:(j-1)){
    k <- k+1
    y1 <- magnetic[-j]
    x1 <- chemical[-j]
    x <- x1[-i]
    y <- y1[-i]


    J1 <- lm(y ~ x)
    es1_i <- J1$coef[1] + J1$coef[2] * chemical[i]
    es1_j <- J1$coef[1] + J1$coef[2] * chemical[j]
    e1[k] <- abs(magnetic[i] - es1_i)+abs(magnetic[j] - es1_j)

    J2 <- lm(y ~ x + I(x^2))
    es2_i <- J2$coef[1] + J2$coef[2] * chemical[i] +
J2$coef[3] * chemical[i]^2
    es2_j <- J2$coef[1] + J2$coef[2] * chemical[j] +
J2$coef[3] * chemical[j]^2
    e2[k] <- abs(magnetic[i] - es2_i)+abs(magnetic[j] - es2_j)

    J3 <- lm(log(y) ~ x)
    es3_i <- exp(J3$coef[1] + J3$coef[2] * chemical[i])
    es3_j <- exp(J3$coef[1] + J3$coef[2] * chemical[j])
    e3[k] <- abs(magnetic[i] - es3_i)+abs(magnetic[j] - es3_j)

    J4 <- lm(log(y) ~ log(x))
    es4_i <- exp(J4$coef[1] + J4$coef[2] * log(chemical[i]))
    es4_j <- exp(J4$coef[1] + J4$coef[2] * log(chemical[j]))
    e4[k] <- abs(magnetic[i] - es4_i)+abs(magnetic[j] - es4_j)
  }
}


c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))


```


According to the prediction error criterion, Model 2, the quadratic model,
would be the best fit for the data.The result is same to the result using leave-one-out (n-fold) cross validation.



# 2020-11-10


### Question 1

 The Count 5 test for equal variances in Section 6.4 is based on the maximum
number of extreme points. Example 6.15 shows that the Count 5 criterion
is not applicable for unequal sample sizes. Implement a permutation test for
equal variance based on the maximum number of extreme points that applies
when sample sizes are not necessarily equal.

### Answer 


According to the article of  McGrath and Yeh,the method can change like that
\[
m_x=\frac{\ln(\frac \alpha 2)}{\ln n_x-\ln(n_x+n_y)}
\]
\[
m_y=\frac{\ln(\frac \alpha 2)}{\ln n_y-\ln(n_x+n_y)}
\]
$n_x$is the sample size of $X$,$n_y$is the sample size of $Y$.If and only if the power $a_x<m_x$ and $a_y<m_y$. accept $H_0$.$a_x$ is the number of extreme points of Sample $X$,$a_y$ is the number of extreme points of Sample $Y$

```{r}
set.seed(20077)
alpha <- 0.05
B <- 10000
n_X <- 20
n_Y <- 30


count_test <- function(X_1,X_2,alpha_a) {
n_1 <- length(X_1)
n_2 <- length(X_2)
m_1 <- log(alpha_a/2)/(log(n_1)-log(n_1+n_2))
m_2 <- log(alpha_a/2)/(log(n_2)-log(n_1+n_2))
c_1 <- X_1 - mean(X_1)
c_2 <- X_2 - mean(X_2)
out_1 <- sum(c_1 > max(c_2)) + sum(c_1 < min(c_2))
out_2 <- sum(c_2 > max(c_1)) + sum(c_2 < min(c_1))
# return 1 (reject) or 0 (do not reject H0)
return(max(c(as.integer(out_1>m_1),as.integer(out_2>m_2))))
}

test_re <- numeric(B) 
for(b in 1:B){
  X <- rnorm(n_X,0,1)
  Y <- rnorm(n_Y,0,1)
  test_re[b] <- count_test(X,Y,alpha)
}
p_count <- mean(test_re)
p_count## output the power
```

As we can see,the power is 0.0493<0.5.

### Question 2

Design experiments for evaluating the performance of the NN,
energy, and ball methods in various situations.

(1) Unequal variances and equal expectations

(2) Unequal variances and unequal expectations

(3) Non-normal distributions: t distribution with 1 df (heavy-tailed
distribution), bimodel distribution (mixture of two normal
distributions)

(4) Unbalanced samples (say, 1 case versus 10 controls)

(5) Note: The parameters should be chosen such that the powers
are distinguishable (say, range from 0.3 to 0.8).


### Answer (1)

```{r}
set.seed(20077)
library(energy)
library(Ball)
library(RANN)
library(boot)
B <- 100
m <- 30##sample size of X
n <- 30##sample size of Y
L <- c(m,n)
p_energy <- p_ball<- p_NN <-numeric(B)
alpha <- 0.1


T_n <- function(z, ix, len,k) {
  n_1 <- len[1]; 
  n_2 <- len[2]; 
  n <- n_1 + n_2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1)
  block_1 <- NN$nn.idx[1:n_1,-1]
  block_2 <- NN$nn.idx[(n_1+1):n,-1]
  count_1 <- sum(block_1 < n_1 + 0.5); 
  count_2 <- sum(block_2 > n_1+0.5)
  (count_1 + count_2) / (k * n)
}



for(b in 1:B){
  X <- rnorm(m,0.3,1)
  Y <- rnorm(n,0.8,1)
  Z <- c(X,Y)
  boot.obs <- eqdist.etest(Z, sizes=L, R=99)
  p_energy[b] <- boot.obs$p.value
  p_ball[b] <- bd.test(x = X, y = Y,num.permutations=99)$p.value

  boot.obj <- boot(data = Z, statistic = T_n, R = 99,
  sim = "permutation", len = L,k=3)
  test_re <- c(boot.obj$t0,boot.obj$t)
  p_NN[b] <- mean(test_re>=test_re[1])
}

print(power_NN<-mean(p_NN<alpha))##power of NN method
print(power_energy<-mean(p_energy<alpha))##  power of energy method
print(power_ball<-mean(p_ball<alpha))## power of ball method
```


### Answer (2)

```{r}
set.seed(20077)
library(energy)
library(Ball)
library(RANN)
library(boot)
B <- 100
m <- 30##sample size of X
n <- 30##sample size of Y
L <- c(m,n)
p_energy <- p_ball<- p_NN <-numeric(B)
alpha <- 0.1


T_n <- function(z, ix, len,k) {
  n_1 <- len[1]; 
  n_2 <- len[2]; 
  n <- n_1 + n_2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1)
  block_1 <- NN$nn.idx[1:n_1,-1]
  block_2 <- NN$nn.idx[(n_1+1):n,-1]
  count_1 <- sum(block_1 < n_1 + 0.5); 
  count_2 <- sum(block_2 > n_1+0.5)
  (count_1 + count_2) / (k * n)
}



for(b in 1:B){
  X <- rnorm(m,0.3,2)
  Y <- rnorm(n,0.8,1.5)
  Z <- c(X,Y)
  boot.obs <- eqdist.etest(Z, sizes=L, R=99)
  p_energy[b] <- boot.obs$p.value
  p_ball[b] <- bd.test(x = X, y = Y,num.permutations=99)$p.value

  boot.obj <- boot(data = Z, statistic = T_n, R = 99,
  sim = "permutation", len = L,k=3)
  test_re <- c(boot.obj$t0,boot.obj$t)
  p_NN[b] <- mean(test_re>=test_re[1])
}

print(power_NN<-mean(p_NN<alpha))##power of NN method
print(power_energy<-mean(p_energy<alpha))##  power of energy method
print(power_ball<-mean(p_ball<alpha))## power of ball method
```


### Answer (3)


```{r}
set.seed(20077)
library(energy)
library(Ball)
library(RANN)
library(boot)
B <- 100
m <- 30##sample size of X
n <- 30##sample size of Y
L <- c(m,n)
p_energy <- p_ball<- p_NN <-numeric(B)
alpha <- 0.1


T_n <- function(z, ix, len,k) {
  n_1 <- len[1]; 
  n_2 <- len[2]; 
  n <- n_1 + n_2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1)
  block_1 <- NN$nn.idx[1:n_1,-1]
  block_2 <- NN$nn.idx[(n_1+1):n,-1]
  count_1 <- sum(block_1 < n_1 + 0.5); 
  count_2 <- sum(block_2 > n_1+0.5)
  (count_1 + count_2) / (k * n)
}



for(b in 1:B){
  X <- rt(m,df=1)
  Y_1 <- rnorm(n,0,1)
  Y_2 <- rnorm(n,0,0.8)
  Y <- numeric(n)
  p <- runif(n)
  for(i in 1:n){
    if(p[i]<0.5) {Y[i]=Y_1[i]}
    else{Y[i]=Y_2[i]}
  }##generate mixture of two normal   distributions
  Z <- c(X,Y)
  boot.obs <- eqdist.etest(Z, sizes=L, R=99)
  p_energy[b] <- boot.obs$p.value
  p_ball[b] <- bd.test(x = X, y = Y,num.permutations=99)$p.value

  boot.obj <- boot(data = Z, statistic = T_n, R = 99,
  sim = "permutation", len = L,k=3)
  test_re <- c(boot.obj$t0,boot.obj$t)
  p_NN[b] <- mean(test_re>=test_re[1])
}

print(power_NN<-mean(p_NN<alpha))##power of NN method
print(power_energy<-mean(p_energy<alpha))##  power of energy method
print(power_ball<-mean(p_ball<alpha))## power of ball method
```


### Answer (4)

```{r}
set.seed(20077)
library(energy)
library(Ball)
library(RANN)
library(boot)
B <- 10
m <- 10##sample size of X
n <- 100##sample size of Y
L <- c(m,n)
p_energy <- p_ball<- p_NN <-numeric(B)
alpha <- 0.1

T_n <- function(z, ix, len,k) {
  n_1 <- len[1]; 
  n_2 <- len[2]; 
  n <- n_1 + n_2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1)
  block_1 <- NN$nn.idx[1:n_1,-1]
  block_2 <- NN$nn.idx[(n_1+1):n,-1]
  count_1 <- sum(block_1 < n_1 + 0.5); 
  count_2 <- sum(block_2 > n_1+0.5)
  (count_1 + count_2) / (k * n)
}



for(b in 1:B){
  X <- rnorm(m,0,1)
  Y <- rnorm(n,0,1)
  Z <- c(X,Y)
  boot.obs <- eqdist.etest(Z, sizes=L, R=999)
  p_energy[b] <- boot.obs$p.value
  p_ball[b] <- bd.test(x = X, y = Y,num.permutations=999)$p.value

  boot.obj <- boot(data = Z, statistic = T_n, R = 999,
  sim = "permutation", len = L,k=3)
  test_re <- c(boot.obj$t0,boot.obj$t)
  p_NN[b] <- mean(test_re>=test_re[1])
}

print(power_NN<-mean(p_NN<alpha))##power of NN method
print(power_energy<-mean(p_energy<alpha))##  power of energy method
print(power_ball<-mean(p_ball<alpha))## power of ball method
```



# 2020-11-17

### Question 1(9.4)

Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of
each chain.





### Answer 1
The pdf of the standard Laplace distribution is
\[
f_L(x)=\frac 12 e^{-|x|}
\]
So
\[
r(x_t,y)=\frac{f_L(Y)}{f_L(X_t)}=e^{|x_t|-|y|}
\]


```{r}
set.seed(20077)
N <- 2000
Sample_Laplace <- function(n,X_1,sigma){
  p <- runif(n-1)
  Sample <- numeric(n)
  count <- 0
  Sample[1] <- X_1
  for(j in 1:(n-1)){
    delta <- rnorm(1,Sample[j],sigma)
    r <- exp(abs(Sample[j])-abs(delta))
    if (p[j]>r){
      Sample[j+1]<-Sample[j]
      count <- count+1
    }
    else{
      Sample[j+1] <- delta
    }
  }
  acceptrate <- count/n
  return(list(acceptrate=acceptrate,Sample=Sample))
}

sigma <- c(0.2,0.6,1.8,5.4)
L <- length(sigma)
x_1 <- 5
accept_rate <- numeric(L)

LSample1<-Sample_Laplace(N,x_1,sigma[1])
LSample2<-Sample_Laplace(N,x_1,sigma[2])
LSample3<-Sample_Laplace(N,x_1,sigma[3])
LSample4<-Sample_Laplace(N,x_1,sigma[4])


accept_rate[1] <- LSample1$acceptrate
accept_rate[2] <- LSample2$acceptrate
accept_rate[3] <- LSample3$acceptrate
accept_rate[4] <- LSample4$acceptrate


sample1 <- LSample1$Sample
sample2 <- LSample2$Sample
sample3 <- LSample3$Sample
sample4 <- LSample4$Sample



result <- matrix(c(sigma,accept_rate),nrow=L,ncol=2,dimnames=list(c("1","2","3","4"),c("sigma","accept rate")))
result


plot(sample1,type = "l",main="sigma=0.2")
plot(sample2,type = "l",main="sigma=0.6")
plot(sample3,type = "l",main="sigma=1.8")
plot(sample4,type = "l",main="sigma=5.4")

```

In the first plot of Figure  with $σ = 0.2$, the ratios $r(X_t, Y )$ tend to be
large and almost every candidate point is accepted.Chain 1 has not converged
to the target in 2000 iterations.And the rate of acceptance(0.0795) is not in $[0.15,0.5]$.

The second chain generated with
$σ = 0.6$ is not converging apparently.But the rate of acceptance(0.1975) is in $[0.15,0.5]$. 


In the third plot $(σ = 1.8)$ the chain is mixing well and converging to the target
distribution.And the rate of acceptance(0.4430) is in $[0.15,0.5]$.

In the fourth
plot, where $σ = 5.4$, the chain is mixing well and converging to the target
distribution.But the rate of acceptance(0.7405) is not in $[0.15,0.5]$.



### Question 2 

For Exercise 9.4, use the Gelman-Rubin method to monitor
convergence of the chain, and run the chain until it converges
approximately to the target distribution according to $\hat{R}<1.2$.





### Answer 2

```{r}
set.seed(20077)


Gelman_Rubin <- function(psi) {##the function from textbook
psi <- as.matrix(psi)
n <- ncol(psi)
k <- nrow(psi)
psi.means <- rowMeans(psi) 
B <- n * var(psi.means) 
psi.w <- apply(psi, 1, "var") 
W <- mean(psi.w) 
Var_hat <- W*(n-1)/n + (B/n)
R_hat <- Var_hat / W 
return(R_hat)
}


N <- 2000
Sample_Laplace <- function(n,X_1,sigma){
  p <- runif(n-1)
  Sample <- numeric(n)
  Sample[1] <- X_1
  for(j in 1:(n-1)){
    delta <- rnorm(1,Sample[j],sigma)
    r <- exp(abs(Sample[j])-abs(delta))
    if (p[j]>r){
      Sample[j+1]<-Sample[j]
    }
    else{
      Sample[j+1] <- delta
    }
  }
  return(Sample)
}

sigma <- c(0.2,0.6,1.8,5.4)
L <- length(sigma)
x_1 <- 5
accept_rate <- numeric(L)
sample <- matrix(c(0:0),nrow=L,ncol=N)
for(l in 1:L){
  sample[l,] <- Sample_Laplace(N,x_1,sigma[l])
}

b <- 100
psi <- t(apply(sample, 1, cumsum))
psi_row <- nrow(psi)
psi_col <- ncol(psi)
for (i in 1:psi_row)
psi[i,] <- psi[i,] / (1:psi_col)
print(Gelman_Rubin(psi))


#par(mfrow=c(2,2))
for (i in 1:L)
plot(psi[i, (b+1):N], type="l",
xlab=i, ylab=bquote(psi))
#par(mfrow=c(1,1)) #restore default
#plot the sequence of R-hat statistics
Rhat <- rep(0, N)
for (j in (b+1):N)
Rhat[j] <- Gelman_Rubin(psi[,1:j])
plot(Rhat[(b+1):N], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
print(Gelman_Rubin(psi))
```

As we can see from the plot, when $n>1300$,$\hat{R}<1.2$.

### Question 3(11.4) 

Find the intersection points $A(k)$ in $(0, \sqrt{k})$ of the curves
\[
S_{k-1}(a)=P\left(t(k-1)>\sqrt{\frac{a^2(k-1)}{k-a^2}}\right)
\]
and
\[
S_{k}(a)=P\left(t(k)>\sqrt{\frac{a^2k}{k+1-a^2}}\right)
\]
for $k = 4 : 25, 100, 500, 1000$, where $t(k)$ is a Student t random variable with
$k$ degrees of freedom.


### Answer 3

```{r}
set.seed(20077)
df <- c(4:25,100,500,1000)
L <- length(df)
root_Bisection <- root_Brent <- numeric(L)
N <- 1000


h <- function(a,k){##the function need to get the root 
  re <- 0
  if(k<2){re <- 0}
  else if(k<=a){re <- 0}
  else if(a<=0){re <- 0}
  else{
    re <-(1-pt(sqrt(a*a*(k-1)/(k-a^2)),df=k-1))-(1-pt(sqrt(a*a*k/(k+1-a^2)),df=k))
  }
  return(re)
}




for(i in 1:L){
  low <- 0.000001
  up <- sqrt(df[i])-0.000001
  root_Brent[i] <- uniroot(function(a) {(1-pt(sqrt(a*a*(df[i]-1)/(df[i]-a^2)),df=df[i]-1))-(1-pt(sqrt(a*a*df[i]/(df[i]+1-a^2)),df=df[i]))},
interval = c(0.000001, sqrt(df[i])-0.000001))$root
  
  y <- numeric(3)
  y[1] <- h(low,df[i])
  y[3] <- h(up,df[i])
  if(y[1]*y[3]>0){
    root_Bisection[i] <- sqrt(df[i])-0.000001
  }
  else{
    count <- 0
    low <- 0.00001
    up <- sqrt(df[i])-0.00001
    epsilon <- 0.00001
    r <- (up+low)/2
    y[2] <- h(r,df[i])
    while(abs(y[2])>epsilon && count < N){
      r <- (up+low)/2
      y[1] <- h(low,df[i])
      y[3] <- h(up,df[i])
      y[2] <- h(r,df[i])
      if(y[1]*y[2]>0){
        low <- r
      }
      else{
        up <- r
      }
      count <- count + 1
    }
    if(count<N){root_Bisection[i] <- r}
    else{root_Bisection[i]<-sqrt(df[i])-0.000001}
  }
}



result <- matrix(c(df,root_Bisection,root_Brent),nrow=L,ncol=3)
colnames(result)<-c("df","Bisection","Brent")
result
```


# 2020-11-24

### Answer 1

It's obvious that
\[
1=p^2+q^2+r^2+2pr+2qr+2pq=(p+q+r)^2
\]
So $r=1-p-q$.

Observed data likelihood
\[
L(p,q;n_{A.},n_{B.},n_{OO},n_{AB})=(p^2+2pr)^{n_{A.}}(q^2+2qr)^{n_{B.}}(r^2)^{n_{OO}}(2pq)^{n_{AB}}
\]

Complete data likelihood
\[
L(p,q;n_{AA},n_{AO},n_{BB},n_{BO},n_{OO},n_{AB})=(p^2)^{n_{AA}}(q^2)^{n_{BB}}(r^2)^{n_{OO}}(2pq)^{n_{AB}}(2pr)^{n_{AO}}(2qr)^{n_{BO}}
\]
\[
l(p,q;n_{AA},n_{AO},n_{BB},n_{BO},n_{OO},n_{AB})\\
=2n_{OO}\ln(r)+n_{AB}\ln(pq)+n_{A.}\ln(pr)+n_{B.}\ln(qr)+n_{AA}\ln(\frac{p}{r})+n_{BB}\ln(\frac{q}{r})+C\\
=(n_{AB}+n_{A.}+n_{AA})\ln(p)+(n_{AB}+n_{B.}+n_{BB})\ln(q)\\+(-n_{AA}+n_{A.}-n_{BB}+n_{B.}+2n_{OO})\ln(r)\\
=l(p,q|n_{AA},n_{A.},n_{BB},n_{B.},n_{OO},n_{AB})
\]
And
\[
n_{AA}|n_{A.},n_{B.},n_{AB},n_{OO}\sim B(n_{A.},\frac{p^2}{p^2+2pr})=B(n_{A.},\frac{p}{p+2r})
\]

\[
n_{BB}|n_{A.},n_{B.},n_{AB},n_{OO}\sim B(n_{B.},\frac{q^2}{q^2+2qr})=B(n_{B.},\frac{q}{q+2r})
\]

E-step
\[
E_{\hat\theta_m}[l(p,q|n_{AA},n_{A.},n_{BB},n_{B.},n_{OO},n_{AB})|n_{A.},n_{B.},n_{AB},n_{OO}]\\=
(n_{AB}+n_{A.}+n_{A.}\frac{\hat p_m}{2-\hat p_m-2\hat q_m})\ln(p)+(n_{AB}+n_{B.}+n_{B.}\frac{\hat q_m}{2-\hat q_m-2\hat p_m})\ln(q)\\+(-n_{A.}\frac{\hat p_m}{2-\hat p_m-2\hat q_m}+n_{A.}-n_{B.}\frac{\hat q_m}{2-\hat q_m-2\hat p_m}+n_{B.}+2n_{OO})\ln(r)\\
=(n_{AB}+n_{A.}\frac{2-2\hat q_m}{2-\hat p_m-2\hat q_m})\ln(p)+(n_{AB}+n_{B.}\frac{2-2\hat p_m}{2-\hat q_m-2\hat p_m})\ln(q)\\+(n_{A.}\frac{2-2\hat p_m-2\hat q_m }{2-\hat p_m-2\hat q_m}+n_{B.}\frac{2-2\hat p_m-2\hat q_m }{2-\hat q_m-2\hat p_m}+2n_{OO})\ln(1-p-q)
\]

M-step
\[
\hat p_{m+1}=\frac{n_{AB}}{2n}+\frac{n_{A.}}{n}\frac{1-\hat q_m}{2-\hat p_m-2\hat q_m}
\]

\[
\hat q_{m+1}=\frac{n_{AB}}{2n}+\frac{n_{B.}}{n}\frac{1-\hat p_m}{2-\hat q_m-2\hat p_m}
\]


\[
l_o=\ln L_o=c+2n_{OO}\ln r+n_{AB}\ln pq+n_{A.}\ln (2pr+p^2) +n_{B.}\ln(2qr+q^2)
\]

```{r}
set.seed(20077)

n_At <- 444
n_Bt <- 132
n_OO <- 361
n_AB <- 63
n <- n_At+n_Bt+n_OO+n_AB
  
  
N <- 15
p_hat <- q_hat <- r_hat <- l<- numeric(N)##l is the log-likelihood without constant
p_hat[1] <- 0.2 
q_hat[1] <- 0.3 
r_hat[1] <- 1-p_hat[1]-q_hat[1]
l[1] <- 2*n_OO*log(r_hat[1])+n_AB*log(p_hat[1]*q_hat[1]) +n_At*log(2*p_hat[1]*r_hat[1]+p_hat[1]*p_hat[1])+n_Bt*log(2*q_hat[1]*r_hat[1]+q_hat[1]*q_hat[1])


for(j in 2:N){
  p_hat[j] <- n_AB/(2*n)+n_At/n*(1-q_hat[j-1])/(2*r_hat[j-1]+p_hat[j-1])
  q_hat[j] <- n_AB/(2*n)+n_Bt/n*(1-p_hat[j-1])/(2*r_hat[j-1]+q_hat[j-1])
  r_hat[j] <- 1-p_hat[j]-q_hat[j]
  l[j] <- 2*n_OO*log(r_hat[j])+n_AB*log(p_hat[j]*q_hat[j]) +n_At*log(2*p_hat[j]*r_hat[j]+p_hat[j]*p_hat[j])+n_Bt*log(2*q_hat[j]*r_hat[j]+q_hat[j]*q_hat[j])
} 

re <- matrix(c(p_hat,q_hat,r_hat,l),nrow=N,ncol=4)
colnames(re) <- c("p_hat","q_hat","r_hat","log-likelihood")
re

```

As we can see,

 the corresponding
log-maximum likelihood values (for observed data) are increasing each step.

### Answer 2

```{r}
set.seed(20077)

formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)

l1 <- list
l2 <- list
l3 <- list
l4 <- list
Lm <- list(l1,l2,l3,l4)



for(i in 1:4){
mtcars.list <- split(mtcars, mtcars$cyl)
mylm <- function(x, form, me) lm(form, x, method = me)
Lm[[i]] <- lapply(mtcars.list, mylm, formulas[[i]], "qr")
}
Lm

```

### Answer 3

```{r}
set.seed(20077)
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
sapply(trials,function(x) x[["p.value"]])

```

### Answer 4

The parameter: function, data, function.value

```{r}
set.seed(20077)

new_lapply<-function(x,y,f,f.value,...){
  L <- list(x,y)
  l<-Map(unlist,L)
  re <- vapply(l,f,f.value,...)
  return(re)
}

z_1 <- c(26,43,51,57,28,73)
z_2 <- c(35,60,53,27,48,23)
new_lapply(z_1,z_2,sum,double(1))
```


# 2020-12-1

# Question
(1)  Implement a random walk Metropolis sampler for generating the standard
Laplace distribution . For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of
each chain.(Rcpp function)


(2) Compare the corresponding generated random numbers with
those by the R function you wrote before using the function
“qqplot”.

(3) Campare the computation time of the two functions with the
function “microbenchmark”.

(4) Comments your results.

# Answer 

We can know that if
\[
U,V \text{i.i.d}\sim U(0,1)
\]
then
\[
Z=\sqrt{-2\ln(U)}\sin(2\pi V)\sim N(0,1)
\]
I use this method to generate the sample of normal distribution in Cpp.

### R-code

```{r}
set.seed(20077)
library("Rcpp")
library("microbenchmark")


R_Lap <- function(sigma, x0, N){
  p <- runif(N)
  count <- numeric(N-1)
  Sample <- numeric(N)
  Sample[1] <- x0
 
 for (i in 1:(N-1)) {
  y <- rnorm(1, Sample[i], sigma)
  r <- exp(abs(Sample[i])-abs(y))
  if (p[i] > r) {
    Sample[i+1] <- Sample[i]
    count[i] <- 0
  }  
  else {
    Sample[i+1] <- y
    count[i] <- 1
  }
 }
 sum_count <- sum(count)
 ac <- sum_count/N
 return(list(Sample = Sample, ac = ac))
}


sourceCpp(code='
#include <Rcpp.h>
#include <stdlib.h>
#include <stdio.h>
#include <math.h>
#define PI 3.141592654

using namespace Rcpp;
//[[Rcpp::export]]

double C_runif(double min, double max){
  double x,t;
  double z=RAND_MAX+1.0;
  x = rand()/z;
  t = min+ (max-min)*x;
  return t;
}

//[[Rcpp::export]]

double C_rnorm(double mu,double sigma){
  double U,V;
  double Z,norm;
  U = C_runif(0,1);
  V = C_runif(0,1);
  Z = sqrt(-2.0 * log(U))* sin(2.0 * PI * V);
  norm = mu+Z*sigma;
  return(norm);
}

//[[Rcpp::export]]

NumericVector C_Lap(double sigma,double x0,int N){
NumericVector Sample(N+1);
Sample[0] = x0;
int count=0;
double y,p,r;
for(int i=1; i<N; i++){
  p = C_runif(0,1);
  y = C_rnorm(Sample[i-1],sigma);
  r = exp(fabs(Sample[i-1])-fabs(y));
  if (p <= r) {
    Sample[i]=y;
    count = count+1;
  }
  else{
    Sample[i] = Sample[i-1];
  }
}
double c=count;
Sample[N]=c/N;
return Sample;
} ')## the acceptance rate is the last element in the array


N <- 3000
sigma <- c(0.1,0.6,3.6,12)
L <- length(sigma)
x_1 <- 2
accept_rate <- numeric(L)
C_ac <- numeric(L)

set.seed(20077)
C1 <- C_Lap(sigma[1],x_1,N) 
C_ac[1] <- C1[N+1]
C_Sample1 <- C1[1:N]
set.seed(20077)
C2 <- C_Lap(sigma[2],x_1,N) 
C_ac[2] <- C2[N+1]
C_Sample2 <- C2[1:N]
set.seed(20077)
C3 <- C_Lap(sigma[3],x_1,N) 
C_ac[3] <- C3[N+1]
C_Sample3 <- C3[1:N]
set.seed(20077)
C4 <- C_Lap(sigma[4],x_1,N) 
C_ac[4] <- C4[N+1]
C_Sample4 <- C4[1:N]

print_C_ac  <- matrix(c(sigma,C_ac),nrow=L,ncol=2,dimnames=list(c("1","2","3","4"),c("sigma","accept rate")))
print_C_ac

#par(mfrow=c(2,2)) 
Sample_C = cbind(C_Sample1, C_Sample2, C_Sample3,  C_Sample4)
for (j in 1:4) {
  plot(Sample_C[,j], type="l",
  xlab=bquote(sigma == .(round(sigma[j],3))),
        ylab="X", ylim=range(Sample_C[,j]))
}


R_Sample1 <- R_Lap(sigma[1],x_1,N)$Sample
R_Sample2 <- R_Lap(sigma[2],x_1,N)$Sample
R_Sample3 <- R_Lap(sigma[3],x_1,N)$Sample
R_Sample4 <- R_Lap(sigma[4],x_1,N)$Sample

#par(mfrow=c(2,2))
qqplot(C_Sample1,R_Sample1)
lines(C_Sample1,C_Sample1,type="l",col="red")
qqplot(C_Sample2,R_Sample2)
lines(C_Sample2,C_Sample2,type="l",col="red")
qqplot(C_Sample3,R_Sample3)
lines(C_Sample3,C_Sample3,type="l",col="red")
qqplot(C_Sample4,R_Sample4)
lines(C_Sample4,C_Sample4,type="l",col="red")




a <- sigma[1]
b <- sigma[2]
c <- sigma[3]
d <- sigma[4]
z <- x_1 

microbenchmark::microbenchmark(C_Lap(a,z,N),R_Lap(a,z,N),C_Lap(b,z,N),R_Lap(b,z,N),C_Lap(c,z,N),R_Lap(c,z,N),C_Lap(d,z,N),R_Lap(d,z,N))   
```


### Comment

1 As we can see,if sigma is small ,the Chain 1 from Rcpp function has not converged to the target in 3000 iterations;if sigma is too small or too big,the acceptance rate is not in [0.15,0.5].

2 When sigma become bigger,the bias between C sample and R sample get smaller.

3 According to the result by "microbenchmark",it's obvious that the Rcpp function cost less time than R function.The speed of Rcpp function is more than 10 times of R function.
In conclusion,the Rcpp function can improve the efficiency in this problem.
